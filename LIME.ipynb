{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4768f1-71a7-4a28-a335-ee68ee24b18c",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654b55e-14e6-429e-b3b4-3010c0a75e93",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- This work comes from [this link](https://towardsdatascience.com/interpretable-machine-learning-for-image-classification-with-lime-ea947e82ca13)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8000c5-6690-4cf6-959d-eaed8ac895df",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682027e7-faa2-4f48-a778-9494ebdbb1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform, segmentation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics\n",
    "import tensorflow as tf\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "import copy\n",
    "from lime import lime_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c385f3-6f29-4248-9cd9-346bd66d4162",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_seed = 222\n",
    "np.random.seed(rnd_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe9ed6-887b-47d7-82fc-1bcb2a438bc2",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa10a6f-0e36-4b65-952d-a2ab5f9e2dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi = io.imread('images/input/Maula.jpg')\n",
    "#Xi = io.imread('images/cat-and-dog.jpeg')\n",
    "Xi = transform.resize(Xi, (299,299))\n",
    "\n",
    "#Inception pre-processing\n",
    "Xi = (Xi - 0.5)*2\n",
    "# Show image before inception preprocessing\n",
    "io.imshow(Xi/2+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e3cd07-64c6-4066-9c77-c319ac8290c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ba1e5-08ec-4e20-bb9f-8b6e5bbd7a25",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d21507-92d9-4917-809b-47096fd94ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use inception model in order to make a prediction. \n",
    "# The output of this model is a vector of length 1000.\n",
    "inceptionV3_model = tf.keras.applications.inception_v3.InceptionV3() #Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac0d69-83e5-46c1-aac9-fc29d4fa5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = inceptionV3_model.predict(Xi[np.newaxis, ...])\n",
    "# Get the position of the 5 classes with highest probability\n",
    "top_pred_classes = preds[0].argsort()[-5:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642b95e9-300d-4e90-80d4-5b65fdb19237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodes the prediction of an ImageNet model. top=5\n",
    "decode_predictions(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bab6d6f-c946-4a2e-ac89-72ec464be15f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Generate random perturbations for input image\n",
    "For the case of images, LIME generates perturbations by turning on and off some of the super-pixels in the image. The following script uses the quick-shift segmentation algorithm to compute the super-pixels in the image. In addition, it generates an array of 150 perturbations where each perturbation is a vector with zeros and ones that represent whether the super-pixel is on or off.\n",
    "\n",
    "The output of this process is a data set of length 150 that will be used to fit an \"interpretable\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf57ded-9a77-4682-9342-dd8d87b90215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate segmentation for image (superpixels)\n",
    "# Each point is the cluster to which the pixel belongs\n",
    "superpixels = segmentation.quickshift(Xi, kernel_size=4, max_dist=200, ratio=0.2)\n",
    "num_superpixels = np.unique(superpixels).shape[0]\n",
    "io.imshow(segmentation.mark_boundaries(Xi/2+0.5, superpixels))\n",
    "print('Number of superpixels: ', num_superpixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba73736-7b5a-4802-9a37-e7a94cce9ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "DPI = fig.get_dpi()\n",
    "fig.set_size_inches(299.0/float(DPI),299.0/float(DPI))\n",
    "plt.imshow(segmentation.mark_boundaries(Xi/2+0.5, superpixels))\n",
    "plt.axis('off')\n",
    "fig.savefig('images/output/maula_segmentation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b159a6-48de-4a39-a237-dd707f5d6062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate perturbations, i.e, obtain a new data set.\n",
    "num_perturb = 150\n",
    "perturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd31cb-0e56-4717-abe3-de685bb57865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to apply perturbations to images\n",
    "# img: an image\n",
    "# perturbation: a mask indicating which superpixes have to be activated\n",
    "# segments: a 299 x 299 numpy array that contains the mapping between the pixel and the superpixel the pixel belongs to\n",
    "def perturb_image(img,perturbation,segments): \n",
    "    active_pixels = np.where(perturbation == 1)[0]\n",
    "    mask = np.zeros(segments.shape)\n",
    "    for active in active_pixels:\n",
    "        mask[segments == active] = 1 \n",
    "    perturbed_image = copy.deepcopy(img)\n",
    "    perturbed_image = perturbed_image*mask[..., np.newaxis]\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a95ef-6181-4299-bc4e-a009e378fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pert = perturbations[1] \n",
    "print(pert) \n",
    "io.imshow(perturb_image(Xi/2+0.5,pert,superpixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d82af6-853e-4a9a-b57a-1bebdc132abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "DPI = fig.get_dpi()\n",
    "fig.set_size_inches(299.0/float(DPI),299.0/float(DPI))\n",
    "plt.imshow(perturb_image(Xi/2+0.5,pert,superpixels))\n",
    "plt.axis('off')\n",
    "fig.savefig('images/output/maula_occlusion.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d7388c-c64a-4f86-89ed-ef414fe202d2",
   "metadata": {},
   "source": [
    "## Step 2: Predict class for perturbations\n",
    "The following script uses the inceptionV3_model to predict the class of each of the perturbed images. The shape of the predictions is (150,1000) which means that for each of the 150 images, we get the probability of belonging to the 1,000 classes in InceptionV3. From these 1,000 classes we will use only the Labrador class in further steps since it is the prediction we want to explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aab317-f0c8-46c6-85f9-3f5ba17acad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for pert in perturbations:\n",
    "    perturbed_img = perturb_image(Xi,pert,superpixels)\n",
    "    pred = inceptionV3_model.predict(perturbed_img[np.newaxis, ...])\n",
    "    predictions.append(pred)\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5b68cb-239c-4eab-94c4-6265a36ee4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_predictions(predictions.squeeze(axis=1))[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ccc537-8034-481c-bf1b-dafeaaac660b",
   "metadata": {},
   "source": [
    "### Step 3: Compute weights (importance) for the perturbations\n",
    "We use a distance metric to evaluate how far is each perturbation from the original image. The original image is just a perturbation with all the super-pixels active (all elements in one). Given that the perturbations are multidimensional vectors, the cosine distance is a metric that can be used for this purpose. After the cosine distance has been computed, a kernel function is used to translate such distance to a value between zero and one (a weight). At the end of this process we have a weight (importance) for each perturbation in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20f007-a6a6-4548-b00f-9f6cfab9806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distances to original image\n",
    "# Perturbation with all superpixels enabled\n",
    "original_image = np.ones(num_superpixels)[np.newaxis, ...] \n",
    "print(original_image.shape)\n",
    "distances = sklearn.metrics.pairwise_distances(perturbations,original_image, metric='cosine').ravel()\n",
    "print(distances.shape)\n",
    "\n",
    "#Transform distances to a value between 0 an 1 (weights) using a kernel function\n",
    "kernel_width = 0.25\n",
    "weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2)) #Kernel function\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342c321-5bbf-4fef-b309-946fffa1d49c",
   "metadata": {},
   "source": [
    "## Step 4: Fit a explainable linear model using the perturbations, predictions and weights\n",
    "We fit a weighted linear model using the information obtained in the previous steps. We get a coefficient for each super-pixel in the image that represents how strong is the effect of the super-pixel in the prediction of Labrador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc3ad6-9154-491a-8ff0-6506997e6fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labrador class is the one we are interested in explaining\n",
    "class_to_explain = top_pred_classes[0]\n",
    "\n",
    "# Fitting a linear model using as target the probability of being the Labrador class for each perturbation\n",
    "simpler_model = LinearRegression()\n",
    "simpler_model.fit(X=perturbations, y=predictions[:,:,class_to_explain], sample_weight=weights)\n",
    "# Getting the coefficients\n",
    "coeff = simpler_model.coef_[0]\n",
    "\n",
    "# Use coefficients from linear model to extract top features\n",
    "num_top_features = 10\n",
    "top_features = np.argsort(coeff)[-num_top_features:] \n",
    "\n",
    "# Show only the superpixels corresponding to the top features\n",
    "mask = np.zeros(num_superpixels)\n",
    "# Activate top superpixels\n",
    "mask[top_features]= True\n",
    "io.imshow(perturb_image(Xi/2+0.5,mask,superpixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a44310-f6d9-4e04-a53c-ca43d1cad0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "DPI = fig.get_dpi()\n",
    "fig.set_size_inches(299.0/float(DPI),299.0/float(DPI))\n",
    "plt.imshow(perturb_image(Xi/2+0.5,mask,superpixels))\n",
    "plt.axis('off')\n",
    "fig.savefig('images/output/maula_lime.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d0dfc-b340-4282-9064-fa7ea2584702",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17304400-cf3c-443a-9f84-d34cc9b52336",
   "metadata": {},
   "source": [
    "## Using lime library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbe20b-33a7-4b4b-b076-dc785f266a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36032f3d-3d7d-4e1c-a823-b041fb05ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = explainer.explain_instance(image=Xi,\n",
    "                                         classifier_fn=inceptionV3_model.predict,  \n",
    "                                         top_labels=1,\n",
    "                                         num_samples=150,\n",
    "                                         random_seed=rnd_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793bf81e-fc6c-41f4-b9af-b06a613572f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_1, mask_1 = explanation.get_image_and_mask(label=explanation.top_labels[0],\n",
    "                                                positive_only=True, \n",
    "                                                num_features=num_top_features,\n",
    "                                                hide_rest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325d592-108b-41fb-a471-cbaa07841871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It may be that the explanation we observe here is different from the previous one \n",
    "# due to the segmentation algorithm (superpixels)\n",
    "io.imshow(segmentation.mark_boundaries(temp_1/2+0.5, mask_1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
